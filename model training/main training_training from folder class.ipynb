{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ztvxCR3N62UOUjsxPEwFQL9Ah90D3-Ph","authorship_tag":"ABX9TyPbCZf33Jb2F6zrjEy4vSqG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"en5RR4G-gG7L"},"outputs":[],"source":["from tensorflow.compat.v1 import ConfigProto\n","from tensorflow.compat.v1 import InteractiveSession\n","\n","config = ConfigProto()\n","config.gpu_options.per_process_gpu_memory_fraction = 0.5\n","config.gpu_options.allow_growth = True\n","session = InteractiveSession(config=config)"]},{"cell_type":"code","source":["# import the libraries as shown below\n","\n","from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.inception_v3 import InceptionV3\n","#from keras.applications.vgg16 import VGG16\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.preprocessing import image\n","from keras_preprocessing.image import ImageDataGenerator,load_img\n","from keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","#import matplotlib.pyplot as plt"],"metadata":{"id":"O37Vfwf_gMxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# re-size all the images to this\n","IMAGE_SIZE = [224, 224]\n","\n","train_path = '/content/drive/MyDrive/| LASSET | Dataset |/Cloud Segmentation 1/ref_cloud_cover_detection_challenge_v1_train_source'\n","valid_path = '/content/drive/MyDrive/Colab Notebooks/Image/validate'"],"metadata":{"id":"oG2y9eGUge6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n","# Here we will be using imagenet weights\n","\n","inception = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n"],"metadata":{"id":"O8uAzsjdgmF7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# don't train existing weights\n","for layer in inception.layers:\n","    layer.trainable = False"],"metadata":{"id":"0-RaDGE1guGz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  # useful for getting number of output classes\n","folders = glob('/content/drive/MyDrive/| LASSET | Dataset |/Cloud Segmentation 1/ref_cloud_cover_detection_challenge_v1_train_source*')"],"metadata":{"id":"vCaF9Rath7Ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# our layers - you can add more if you want\n","x = Flatten()(inception.output)"],"metadata":{"id":"1OmVlNBHh-XN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = Dense(len(folders), activation='softmax')(x)\n","\n","# create a model object\n","model = Model(inputs=inception.input, outputs=prediction)"],"metadata":{"id":"n1t3-ERtiBLk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tell the model what cost and optimization method to use\n","model.compile(\n","  loss='categorical_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")"],"metadata":{"id":"Dvj-vd8viK18"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use the Image Data Generator to import the images from the dataset\n","from keras_preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)"],"metadata":{"id":"feyTYfXUiOoU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make sure you provide the same target size as initialied for the image size\n","training_set = train_datagen.flow_from_directory( target_size=(224,224),directory='/content/drive/MyDrive/| LASSET | Dataset |/Cloud Segmentation 1/ref_cloud_cover_detection_challenge_v1_train_source',\n","                                                 batch_size = 2,\n","                                                 )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xA6N6OhqiWG3","executionInfo":{"status":"ok","timestamp":1672750719431,"user_tz":-360,"elapsed":380531,"user":{"displayName":"FAHAD BIN ABDULLAH","userId":"05100949299404044263"}},"outputId":"aaa98cc2-ccff-4fa0-c810-9f46aa34e785"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6106 images belonging to 11748 classes.\n"]}]},{"cell_type":"code","source":["test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/Colab Notebooks/Image/validate',\n","                                            target_size = (224, 224),\n","                                            batch_size = 32,\n","                                            class_mode = 'categorical')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBJZV3r3iZek","executionInfo":{"status":"ok","timestamp":1672749260506,"user_tz":-360,"elapsed":444,"user":{"displayName":"FAHAD BIN ABDULLAH","userId":"05100949299404044263"}},"outputId":"3fb6e3c4-0219-46f1-f499-008d50d5bcb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 0 images belonging to 0 classes.\n"]}]},{"cell_type":"code","source":["# fit the model\n","# Run the cell. It will take some time to execute\n","r = model.fit_generator(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=10,\n","  steps_per_epoch=len(training_set),\n","  validation_steps=len(test_set)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"Ysj-6lwhifgr","executionInfo":{"status":"error","timestamp":1672748096648,"user_tz":-360,"elapsed":553,"user":{"displayName":"FAHAD BIN ABDULLAH","userId":"05100949299404044263"}},"outputId":"8576e121-b039-4801-8da8-8aa2c4191df5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-04ffb29a66a7>:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  r = model.fit_generator(\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-04ffb29a66a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Run the cell. It will take some time to execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m r = model.fit_generator(\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2260\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2261\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       raise ValueError('Asked to retrieve element {idx}, '\n\u001b[0m\u001b[1;32m    101\u001b[0m                        \u001b[0;34m'but the Sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                        'has length {length}'.format(idx=idx, length=len(self)))\n","\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"BMxrJHf4ivhe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the loss\n","plt.plot(r.history['loss'], label='train loss')\n","plt.plot(r.history['val_loss'], label='val loss')\n","plt.legend()\n","plt.show()\n","plt.savefig('LossVal_loss')\n","\n","# plot the accuracy\n","plt.plot(r.history['accuracy'], label='train acc')\n","plt.plot(r.history['val_accuracy'], label='val acc')\n","plt.legend()\n","plt.show()\n","plt.savefig('AccVal_acc')"],"metadata":{"id":"noXZak1Kiwk5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save it as a h5 file\n","\n","\n","from tensorflow.keras.models import load_model\n","\n","model.save('model_inception.h5')"],"metadata":{"id":"RxRNiYk-i0A-"},"execution_count":null,"outputs":[]}]}